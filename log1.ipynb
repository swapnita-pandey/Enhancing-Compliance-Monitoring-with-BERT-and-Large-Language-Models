{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16007 entries, 0 to 16006\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   IP      16007 non-null  object\n",
      " 1   Time    16007 non-null  object\n",
      " 2   URL     16007 non-null  object\n",
      " 3   Staus   16007 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 500.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the \"weblog.csv\" dataset\n",
    "dataset_path = \"weblog.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Step 2: Inspect the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           IP                   Time  \\\n",
      "0  10.128.2.1  [29/Nov/2017:06:58:55   \n",
      "1  10.128.2.1  [29/Nov/2017:06:59:02   \n",
      "2  10.128.2.1  [29/Nov/2017:06:59:03   \n",
      "3  10.131.2.1  [29/Nov/2017:06:59:04   \n",
      "4  10.130.2.1  [29/Nov/2017:06:59:06   \n",
      "\n",
      "                                             URL Status  \n",
      "0                        GET /login.php HTTP/1.1    200  \n",
      "1                     POST /process.php HTTP/1.1    302  \n",
      "2                         GET /home.php HTTP/1.1    200  \n",
      "3          GET /js/vendor/moment.min.js HTTP/1.1    200  \n",
      "4  GET /bootstrap-3.3.7/js/bootstrap.js HTTP/1.1    200  \n"
     ]
    }
   ],
   "source": [
    "# Rename the \"Staus\" column to \"Status\"\n",
    "df.rename(columns={\"Staus\": \"Status\"}, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the dataset:\n",
      "           IP                   Time  \\\n",
      "0  10.128.2.1  [29/Nov/2017:06:58:55   \n",
      "1  10.128.2.1  [29/Nov/2017:06:59:02   \n",
      "2  10.128.2.1  [29/Nov/2017:06:59:03   \n",
      "3  10.131.2.1  [29/Nov/2017:06:59:04   \n",
      "4  10.130.2.1  [29/Nov/2017:06:59:06   \n",
      "\n",
      "                                             URL Status  \n",
      "0                        GET /login.php HTTP/1.1    200  \n",
      "1                     POST /process.php HTTP/1.1    302  \n",
      "2                         GET /home.php HTTP/1.1    200  \n",
      "3          GET /js/vendor/moment.min.js HTTP/1.1    200  \n",
      "4  GET /bootstrap-3.3.7/js/bootstrap.js HTTP/1.1    200  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 3: Data Cleaning and Handling Missing Values\n",
    "df.dropna(inplace=True)  # Remove rows with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        29/Nov/2017:06:58:55\n",
       "1        29/Nov/2017:06:59:02\n",
       "2        29/Nov/2017:06:59:03\n",
       "3        29/Nov/2017:06:59:04\n",
       "4        29/Nov/2017:06:59:06\n",
       "                 ...         \n",
       "16002    02/Mar/2018:15:47:12\n",
       "16003    02/Mar/2018:15:47:23\n",
       "16004    02/Mar/2018:15:47:32\n",
       "16005    02/Mar/2018:15:47:35\n",
       "16006    02/Mar/2018:15:47:46\n",
       "Name: Time, Length: 16007, dtype: object"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Time'] = df['Time'].str.replace('[', '', regex=False)\n",
    "df['Time']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        29/Nov/2017\n",
       "1        29/Nov/2017\n",
       "2        29/Nov/2017\n",
       "3        29/Nov/2017\n",
       "4        29/Nov/2017\n",
       "            ...     \n",
       "16002    02/Mar/2018\n",
       "16003    02/Mar/2018\n",
       "16004    02/Mar/2018\n",
       "16005    02/Mar/2018\n",
       "16006    02/Mar/2018\n",
       "Name: Date, Length: 16007, dtype: object"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_components = df['Time'].str.split(':', expand=True)\n",
    "df['Date'] = time_components[0]\n",
    "df['Date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        06\n",
       "1        06\n",
       "2        06\n",
       "3        06\n",
       "4        06\n",
       "         ..\n",
       "16002    15\n",
       "16003    15\n",
       "16004    15\n",
       "16005    15\n",
       "16006    15\n",
       "Name: TimeOfDay, Length: 16007, dtype: object"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TimeOfDay'] = time_components[1]\n",
    "df['TimeOfDay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Time' column\n",
    "df.drop(columns=['Time'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode URLs\n",
    "df['URL_Encoded'] = label_encoder.fit_transform(df['URL'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode HTTP methods\n",
    "df['Method_Encoded'] = label_encoder.fit_transform(df['URL'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed dataset:\n",
      "           IP                                            URL Status  \\\n",
      "0  10.128.2.1                        GET /login.php HTTP/1.1    200   \n",
      "1  10.128.2.1                     POST /process.php HTTP/1.1    302   \n",
      "2  10.128.2.1                         GET /home.php HTTP/1.1    200   \n",
      "3  10.131.2.1          GET /js/vendor/moment.min.js HTTP/1.1    200   \n",
      "4  10.130.2.1  GET /bootstrap-3.3.7/js/bootstrap.js HTTP/1.1    200   \n",
      "\n",
      "          Date TimeOfDay  URL_Encoded  Method_Encoded  \n",
      "0  29/Nov/2017        06          193             193  \n",
      "1  29/Nov/2017        06          308             308  \n",
      "2  29/Nov/2017        06          182             182  \n",
      "3  29/Nov/2017        06          191             191  \n",
      "4  29/Nov/2017        06           40              40  \n"
     ]
    }
   ],
   "source": [
    "# Display preprocessed data\n",
    "print(\"\\nPreprocessed dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining compliance rules and standards is a critical step in your project. You need to translate these rules into a structured format that can be understood by your system for automated analysis. Let's break down this step:\n",
    "\n",
    "Define Compliance Rules: Before writing code, make sure you have a clear understanding of the compliance rules and standards that you want to enforce. These could be related to data security, access controls, user privileges, system configurations, and more.\n",
    "\n",
    "Structured Format: You can represent each compliance rule in your dataset using a dictionary structure or a DataFrame. Each rule could have attributes like the rule description, the conditions to check, associated keywords, categories, and priority levels.\n",
    "\n",
    "Categorization and Priority: If your organization has different categories of compliance rules (e.g., security, privacy, operational), you can organize them accordingly. Similarly, you can assign priority levels to each rule to determine the severity of non-compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Rule Description  \\\n",
      "0  Ensure strong passwords are used   \n",
      "1     Monitor failed login attempts   \n",
      "2      Restrict unauthorized access   \n",
      "\n",
      "                                          Conditions  Keywords  \\\n",
      "0                    Password length >= 8 characters  password   \n",
      "1           Failed login attempts >= 5 within 1 hour     login   \n",
      "2  Access to sensitive data requires multi-factor...    access   \n",
      "\n",
      "         Category Priority  \n",
      "0        Security     High  \n",
      "1        Security   Medium  \n",
      "2  Access Control     High  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define compliance rules in a DataFrame\n",
    "compliance_rules = pd.DataFrame({\n",
    "    'Rule Description': [\n",
    "        'Ensure strong passwords are used',\n",
    "        'Monitor failed login attempts',\n",
    "        'Restrict unauthorized access'\n",
    "    ],\n",
    "    'Conditions': [\n",
    "        'Password length >= 8 characters',\n",
    "        'Failed login attempts >= 5 within 1 hour',\n",
    "        'Access to sensitive data requires multi-factor authentication'\n",
    "    ],\n",
    "    'Keywords': [\n",
    "        'password', 'login', 'access'\n",
    "    ],\n",
    "    'Category': [\n",
    "        'Security', 'Security', 'Access Control'\n",
    "    ],\n",
    "    'Priority': [\n",
    "        'High', 'Medium', 'High'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display the compliance rules DataFrame\n",
    "print(compliance_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the large language model (LLM) depends on the specific requirements of your project and the nature of the text data you're working with. bert-base-uncased is a commonly used LLM that is pretrained on a large corpus of text and is a good starting point for many NLP tasks. It's designed for uncased text, meaning all the text is lowercased before training, which can be helpful for tasks where the case of the words might not matter much.\n",
    "\n",
    "However, there are several other pre-trained LLMs available through the Hugging Face Transformers library that you can consider based on your project's requirements. Some alternatives include:\n",
    "\n",
    "bert-base-cased: Similar to bert-base-uncased, but retains the case of the text. This might be suitable if preserving the original case is important for your task.\n",
    "\n",
    "roberta-base: This is another popular model that is based on the BERT architecture but uses a modified training approach. It has shown strong performance on various NLP benchmarks.\n",
    "\n",
    "distilbert-base-uncased: This is a smaller and faster version of BERT that still provides competitive performance on various tasks. It's useful when computational resources are limited.\n",
    "\n",
    "xlnet-base-cased: XLNet is another architecture that has achieved state-of-the-art results on multiple NLP benchmarks. It's known for its autoregressive training approach.\n",
    "\n",
    "gpt2: While originally designed for text generation, GPT-2 can also be fine-tuned for classification tasks. It might be useful if you're interested in text generation alongside compliance classification.\n",
    "\n",
    "The choice of LLM should take into account factors such as model size, training data, computational resources, and task-specific requirements. It's recommended to experiment with different models and evaluate their performance on your specific dataset to determine the best fit for your compliance monitoring task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define your LLM and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['URL'], df['Status'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['302' '200' '304' '206' 'No' '404' '2018]' '2017]' 'dumped' 'Aborted'\n",
      " 'Assertion' 'Segmentation' 'found']\n",
      "['200' '302' '304' 'No' '404' '2018]' '206' '2017]' 'Assertion' 'dumped']\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.unique())\n",
    "print(test_labels.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map compliance labels to numerical values\n",
    "label_mapping = {'non-compliant': 0, 'compliant': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply label encoding to the 'Status' column (or the correct column name)\n",
    "df['Status'] = df['Status'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove instances with unexpected labels (if needed)\n",
    "valid_labels = ['non-compliant', 'compliant']\n",
    "df = df[df['Status'].isin(valid_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IP</th>\n",
       "      <th>URL</th>\n",
       "      <th>Status</th>\n",
       "      <th>Date</th>\n",
       "      <th>TimeOfDay</th>\n",
       "      <th>URL_Encoded</th>\n",
       "      <th>Method_Encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [IP, URL, Status, Date, TimeOfDay, URL_Encoded, Method_Encoded]\n",
       "Index: []"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[173], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Split the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_texts, test_texts, train_labels, test_labels \u001b[39m=\u001b[39m train_test_split(df[\u001b[39m'\u001b[39;49m\u001b[39mURL\u001b[39;49m\u001b[39m'\u001b[39;49m], df[\u001b[39m'\u001b[39;49m\u001b[39mStatus\u001b[39;49m\u001b[39m'\u001b[39;49m], test_size\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2617\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2614\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2616\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2617\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2618\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2619\u001b[0m )\n\u001b[0;32m   2621\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2622\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2273\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2270\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[0;32m   2272\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2273\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2274\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2275\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2276\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2277\u001b[0m     )\n\u001b[0;32m   2279\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['URL'], df['Status'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Convert labels to tensors\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(train_labels\u001b[39m.\u001b[39;49mtolist())\n\u001b[0;32m      5\u001b[0m test_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(test_labels\u001b[39m.\u001b[39mtolist())\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_labels.tolist())\n",
    "test_labels = torch.tensor(test_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=500,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_encodings,\n",
    "    eval_dataset=test_encodings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_encodings)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "precision = precision_score(test_labels, predicted_labels)\n",
    "recall = recall_score(test_labels, predicted_labels)\n",
    "f1 = f1_score(test_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
