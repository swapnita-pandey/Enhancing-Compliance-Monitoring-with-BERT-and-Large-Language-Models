{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16007 entries, 0 to 16006\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   IP      16007 non-null  object\n",
      " 1   Time    16007 non-null  object\n",
      " 2   URL     16007 non-null  object\n",
      " 3   Staus   16007 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 500.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the \"weblog.csv\" dataset\n",
    "dataset_path = \"weblog.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Step 2: Inspect the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           IP                   Time  \\\n",
      "0  10.128.2.1  [29/Nov/2017:06:58:55   \n",
      "1  10.128.2.1  [29/Nov/2017:06:59:02   \n",
      "2  10.128.2.1  [29/Nov/2017:06:59:03   \n",
      "3  10.131.2.1  [29/Nov/2017:06:59:04   \n",
      "4  10.130.2.1  [29/Nov/2017:06:59:06   \n",
      "\n",
      "                                             URL Status  \n",
      "0                        GET /login.php HTTP/1.1    200  \n",
      "1                     POST /process.php HTTP/1.1    302  \n",
      "2                         GET /home.php HTTP/1.1    200  \n",
      "3          GET /js/vendor/moment.min.js HTTP/1.1    200  \n",
      "4  GET /bootstrap-3.3.7/js/bootstrap.js HTTP/1.1    200  \n"
     ]
    }
   ],
   "source": [
    "# Rename the \"Staus\" column to \"Status\"\n",
    "df.rename(columns={\"Staus\": \"Status\"}, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IP</th>\n",
       "      <th>Time</th>\n",
       "      <th>URL</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.128.2.1</td>\n",
       "      <td>[29/Nov/2017:06:58:55</td>\n",
       "      <td>GET /login.php HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.128.2.1</td>\n",
       "      <td>[29/Nov/2017:06:59:02</td>\n",
       "      <td>POST /process.php HTTP/1.1</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.128.2.1</td>\n",
       "      <td>[29/Nov/2017:06:59:03</td>\n",
       "      <td>GET /home.php HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.131.2.1</td>\n",
       "      <td>[29/Nov/2017:06:59:04</td>\n",
       "      <td>GET /js/vendor/moment.min.js HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.130.2.1</td>\n",
       "      <td>[29/Nov/2017:06:59:06</td>\n",
       "      <td>GET /bootstrap-3.3.7/js/bootstrap.js HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           IP                   Time  \\\n",
       "0  10.128.2.1  [29/Nov/2017:06:58:55   \n",
       "1  10.128.2.1  [29/Nov/2017:06:59:02   \n",
       "2  10.128.2.1  [29/Nov/2017:06:59:03   \n",
       "3  10.131.2.1  [29/Nov/2017:06:59:04   \n",
       "4  10.130.2.1  [29/Nov/2017:06:59:06   \n",
       "\n",
       "                                             URL Status  \n",
       "0                        GET /login.php HTTP/1.1    200  \n",
       "1                     POST /process.php HTTP/1.1    302  \n",
       "2                         GET /home.php HTTP/1.1    200  \n",
       "3          GET /js/vendor/moment.min.js HTTP/1.1    200  \n",
       "4  GET /bootstrap-3.3.7/js/bootstrap.js HTTP/1.1    200  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows where Status is \"No\"\n",
    "df = df[df[\"Status\"] != \"No\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 15840 entries, 0 to 16006\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   IP      15840 non-null  object\n",
      " 1   Time    15840 non-null  object\n",
      " 2   URL     15840 non-null  object\n",
      " 3   Status  15840 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 618.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Inspect the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First few rows of the dataset:\n",
      "           IP                   Time  \\\n",
      "0  10.128.2.1  [29/Nov/2017:06:58:55   \n",
      "1  10.128.2.1  [29/Nov/2017:06:59:02   \n",
      "2  10.128.2.1  [29/Nov/2017:06:59:03   \n",
      "3  10.131.2.1  [29/Nov/2017:06:59:04   \n",
      "4  10.130.2.1  [29/Nov/2017:06:59:06   \n",
      "\n",
      "                                             URL Status  \n",
      "0                        GET /login.php HTTP/1.1    200  \n",
      "1                     POST /process.php HTTP/1.1    302  \n",
      "2                         GET /home.php HTTP/1.1    200  \n",
      "3          GET /js/vendor/moment.min.js HTTP/1.1    200  \n",
      "4  GET /bootstrap-3.3.7/js/bootstrap.js HTTP/1.1    200  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 3: Data Cleaning and Handling Missing Values\n",
    "df.dropna(inplace=True)  # Remove rows with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        29/Nov/2017:06:58:55\n",
       "1        29/Nov/2017:06:59:02\n",
       "2        29/Nov/2017:06:59:03\n",
       "3        29/Nov/2017:06:59:04\n",
       "4        29/Nov/2017:06:59:06\n",
       "                 ...         \n",
       "16002    02/Mar/2018:15:47:12\n",
       "16003    02/Mar/2018:15:47:23\n",
       "16004    02/Mar/2018:15:47:32\n",
       "16005    02/Mar/2018:15:47:35\n",
       "16006    02/Mar/2018:15:47:46\n",
       "Name: Time, Length: 15840, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Time'] = df['Time'].str.replace('[', '', regex=False)\n",
    "df['Time']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        29/Nov/2017\n",
       "1        29/Nov/2017\n",
       "2        29/Nov/2017\n",
       "3        29/Nov/2017\n",
       "4        29/Nov/2017\n",
       "            ...     \n",
       "16002    02/Mar/2018\n",
       "16003    02/Mar/2018\n",
       "16004    02/Mar/2018\n",
       "16005    02/Mar/2018\n",
       "16006    02/Mar/2018\n",
       "Name: Date, Length: 15840, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_components = df['Time'].str.split(':', expand=True)\n",
    "df['Date'] = time_components[0]\n",
    "df['Date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        06\n",
       "1        06\n",
       "2        06\n",
       "3        06\n",
       "4        06\n",
       "         ..\n",
       "16002    15\n",
       "16003    15\n",
       "16004    15\n",
       "16005    15\n",
       "16006    15\n",
       "Name: TimeOfDay, Length: 15840, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TimeOfDay'] = time_components[1]\n",
    "df['TimeOfDay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Time' column\n",
    "df.drop(columns=['Time'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode URLs\n",
    "df['URL_Encoded'] = label_encoder.fit_transform(df['URL'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode HTTP methods\n",
    "df['Method_Encoded'] = label_encoder.fit_transform(df['URL'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed dataset:\n",
      "           IP                                            URL Status  \\\n",
      "0  10.128.2.1                        GET /login.php HTTP/1.1    200   \n",
      "1  10.128.2.1                     POST /process.php HTTP/1.1    302   \n",
      "2  10.128.2.1                         GET /home.php HTTP/1.1    200   \n",
      "3  10.131.2.1          GET /js/vendor/moment.min.js HTTP/1.1    200   \n",
      "4  10.130.2.1  GET /bootstrap-3.3.7/js/bootstrap.js HTTP/1.1    200   \n",
      "\n",
      "          Date TimeOfDay  URL_Encoded  Method_Encoded  \n",
      "0  29/Nov/2017        06          190             190  \n",
      "1  29/Nov/2017        06          305             305  \n",
      "2  29/Nov/2017        06          179             179  \n",
      "3  29/Nov/2017        06          188             188  \n",
      "4  29/Nov/2017        06           37              37  \n"
     ]
    }
   ],
   "source": [
    "# Display preprocessed data\n",
    "print(\"\\nPreprocessed dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining compliance rules and standards is a critical step in your project. You need to translate these rules into a structured format that can be understood by your system for automated analysis. Let's break down this step:\n",
    "\n",
    "Define Compliance Rules: Before writing code, make sure you have a clear understanding of the compliance rules and standards that you want to enforce. These could be related to data security, access controls, user privileges, system configurations, and more.\n",
    "\n",
    "Structured Format: You can represent each compliance rule in your dataset using a dictionary structure or a DataFrame. Each rule could have attributes like the rule description, the conditions to check, associated keywords, categories, and priority levels.\n",
    "\n",
    "Categorization and Priority: If your organization has different categories of compliance rules (e.g., security, privacy, operational), you can organize them accordingly. Similarly, you can assign priority levels to each rule to determine the severity of non-compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Rule Description  \\\n",
      "0  Ensure strong passwords are used   \n",
      "1     Monitor failed login attempts   \n",
      "2      Restrict unauthorized access   \n",
      "\n",
      "                                          Conditions  Keywords  \\\n",
      "0                    Password length >= 8 characters  password   \n",
      "1           Failed login attempts >= 5 within 1 hour     login   \n",
      "2  Access to sensitive data requires multi-factor...    access   \n",
      "\n",
      "         Category Priority  \n",
      "0        Security     High  \n",
      "1        Security   Medium  \n",
      "2  Access Control     High  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define compliance rules in a DataFrame\n",
    "compliance_rules = pd.DataFrame({\n",
    "    'Rule Description': [\n",
    "        'Ensure strong passwords are used',\n",
    "        'Monitor failed login attempts',\n",
    "        'Restrict unauthorized access'\n",
    "    ],\n",
    "    'Conditions': [\n",
    "        'Password length >= 8 characters',\n",
    "        'Failed login attempts >= 5 within 1 hour',\n",
    "        'Access to sensitive data requires multi-factor authentication'\n",
    "    ],\n",
    "    'Keywords': [\n",
    "        'password', 'login', 'access'\n",
    "    ],\n",
    "    'Category': [\n",
    "        'Security', 'Security', 'Access Control'\n",
    "    ],\n",
    "    'Priority': [\n",
    "        'High', 'Medium', 'High'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display the compliance rules DataFrame\n",
    "print(compliance_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the large language model (LLM) depends on the specific requirements of your project and the nature of the text data you're working with. bert-base-uncased is a commonly used LLM that is pretrained on a large corpus of text and is a good starting point for many NLP tasks. It's designed for uncased text, meaning all the text is lowercased before training, which can be helpful for tasks where the case of the words might not matter much.\n",
    "\n",
    "However, there are several other pre-trained LLMs available through the Hugging Face Transformers library that you can consider based on your project's requirements. Some alternatives include:\n",
    "\n",
    "bert-base-cased: Similar to bert-base-uncased, but retains the case of the text. This might be suitable if preserving the original case is important for your task.\n",
    "\n",
    "roberta-base: This is another popular model that is based on the BERT architecture but uses a modified training approach. It has shown strong performance on various NLP benchmarks.\n",
    "\n",
    "distilbert-base-uncased: This is a smaller and faster version of BERT that still provides competitive performance on various tasks. It's useful when computational resources are limited.\n",
    "\n",
    "xlnet-base-cased: XLNet is another architecture that has achieved state-of-the-art results on multiple NLP benchmarks. It's known for its autoregressive training approach.\n",
    "\n",
    "gpt2: While originally designed for text generation, GPT-2 can also be fine-tuned for classification tasks. It might be useful if you're interested in text generation alongside compliance classification.\n",
    "\n",
    "The choice of LLM should take into account factors such as model size, training data, computational resources, and task-specific requirements. It's recommended to experiment with different models and evaluate their performance on your specific dataset to determine the best fit for your compliance monitoring task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define your LLM and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['URL'], df['Status'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input for Model: The resulting train_encodings and test_encodings are dictionaries containing the encoded sequences that the model can understand. These sequences include the input IDs (tokenized text), attention masks (to indicate which tokens should be attended to), and optionally token type IDs (useful for models like BERT).\n",
    "Now that you have your data tokenized and encoded, you're ready to proceed with the model training process. You can create PyTorch Dataset objects from these encodings and then train the model using the Trainer and TrainingArguments as explained in my previous response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and encode training texts\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Tokenize and encode test texts\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, return_tensors='pt', max_length=128)\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, return_tensors='pt', max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2131, 1013,  ...,    0,    0,    0],\n",
      "        [ 101, 2131, 1013,  ...,    0,    0,    0],\n",
      "        [ 101, 2131, 1013,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2695, 1013,  ...,    0,    0,    0],\n",
      "        [ 101, 2131, 1013,  ...,    0,    0,    0],\n",
      "        [ 101, 2131, 1013,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2131, 1013,  ...,    0,    0,    0],\n",
      "        [ 101, 2131, 1013,  ...,    0,    0,    0],\n",
      "        [ 101, 2131, 1013,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2131, 1013,  ...,    0,    0,    0],\n",
      "        [ 101, 2131, 1013,  ...,    0,    0,    0],\n",
      "        [ 101, 2131, 1013,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to integers (assuming '200' and '302' are your classes)\n",
    "train_labels = train_labels.map({'200': 0, '302': 1})\n",
    "test_labels = test_labels.map({'200': 0, '302': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset objects\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "test_dataset = CustomDataset(test_encodings, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4f5bf3e81f42ad8dab061dfe1d5ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4752 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swapn\\AppData\\Local\\Temp\\ipykernel_21632\\1277991774.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "8992",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 8992",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1539\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1534\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1536\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1538\u001b[0m )\n\u001b[1;32m-> 1539\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1540\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1541\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1542\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1543\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1544\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1787\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1784\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1787\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1788\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1789\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[0;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[54], line 11\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m     10\u001b[0m     item \u001b[39m=\u001b[39m {key: torch\u001b[39m.\u001b[39mtensor(val[idx]) \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodings\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m---> 11\u001b[0m     item[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels[idx])\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m item\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m   1009\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1010\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1115\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1118\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\swapn\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 8992"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions = trainer.predict(test_encodings)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "precision = precision_score(test_labels, predicted_labels)\n",
    "recall = recall_score(test_labels, predicted_labels)\n",
    "f1 = f1_score(test_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
